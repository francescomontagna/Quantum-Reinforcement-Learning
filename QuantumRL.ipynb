{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuantumRL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOtrCcf1cMfkpsZHqZNEnkL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescomontagna/Quantum-Reinforcement-Learning/blob/main/QuantumRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ywYaaLPmtL"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETGJGzcDIxwC"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import cmath\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "from functools import reduce"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWCQSEmAOUI3"
      },
      "source": [
        "# Introduction\n",
        "## Problems\n",
        "- Since we can access the features, and know the transition probability from a state to another, this RL protocol is model based.\n",
        "Note that this is in contrast with the uncertainty about a quantum state from the observator point of view: an observator can only access the collapsed state, having no access to the amplitudes. \n",
        "\n",
        "In the paper Girolami sent me, they explicitly account for this fact, setting up a model free protocol.  \n",
        "\n",
        "\n",
        "## Reward\n",
        "Il principale problema è la formulzione della reward. per ora, l'unica soluzione tale da portare risultati accettabili è stata:\n",
        "- reward = \"grande\" per stato terminal\n",
        "- reward < 0 per stato non terminal  \n",
        "\n",
        "L'aggiunta di reward negativa a punire ogni step che non portasse a uno stato terminale, è stato cruciale. Infatti, ho usato altri tipi di reward, ma tutte fallimentari  \n",
        "- fidelity\n",
        "- fidelity per stato terminale, altrimenti 0\n",
        "- \"grande\" per stato terminale, altrimenti 0  \n",
        "\n",
        "Tutti questi tentativi si sono mostrati fallimentari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_8rsT6Mr7Zy"
      },
      "source": [
        "## Off Policy\n",
        "Sono dovuto ricorrere a un'approccio off policy, in quanto un approccio on policy non garantiva esplorazione sufficiente in uno spazio di ricerca così vasto. I risultati erano frtemente limitati, la matrice dei weight non vedeva mai alcune azioni, il cui valore rimaneva 0 per ogni features. I risultati erano in media peggiori di un' approccio Random.  \n",
        "Andando a usare una bahaviour policy completamente randomica, e una target policy completamente greedy, i risultati sono notavolmente migliorati. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-LCGYpUPpTm"
      },
      "source": [
        "# Quantum Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV8sEgIgZisp"
      },
      "source": [
        "### Qubit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjboXlFWG5EK"
      },
      "source": [
        "class Qubit:\n",
        "  def __init__(self, amplitudes):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      amplitudes (np.array): amplitudes of the |0>, |1> vectors\n",
        "    \"\"\"\n",
        "    error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "    assert math.isclose(reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes)), 1, rel_tol = 1e-2), error\n",
        "    self._amplitudes = amplitudes\n",
        "\n",
        "  def qubit(self):\n",
        "    return self._amplitudes\n",
        "    \n",
        "\n",
        "class Basis(Qubit):\n",
        "  def __init__(self, index):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      index: index of the position = 1\n",
        "    \"\"\"\n",
        "    amplitudes = np.zeros((2, ))\n",
        "    amplitudes[index] += 1\n",
        "    super().__init__(amplitudes)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnWqxby5ZfY_"
      },
      "source": [
        "### Quantum State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRBaWNuZyhgp"
      },
      "source": [
        "class QuantumState:\n",
        "  def __init__(self, amplitudes): \n",
        "\n",
        "    # We assume computational basis\n",
        "    self._basis = {\n",
        "        '00': [Basis(0), Basis(0)],\n",
        "        '01': [Basis(0), Basis(1)],\n",
        "        '10': [Basis(1), Basis(0)],\n",
        "        '11': [Basis(1), Basis(1)],\n",
        "    }\n",
        "\n",
        "    a1, a2, a3, a4 = amplitudes\n",
        "    self._amplitudes = {\n",
        "        '00': a1,\n",
        "        '01': a2,\n",
        "        '10': a3,\n",
        "        '11': a4\n",
        "    }\n",
        "    \n",
        "    self._keys = list(self._basis.keys())\n",
        "\n",
        "    error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "    val = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes))\n",
        "    assert math.isclose(val, 1, rel_tol = 1e-2), error + f\" instead is {val}\"\n",
        "\n",
        "  def get_amplitudes(self):\n",
        "    return np.array(list(self._amplitudes.values()))\n",
        "\n",
        "  def get_features(self):\n",
        "    real = np.real(list(self._amplitudes.values()))\n",
        "    imag = np.imag(list(self._amplitudes.values()))\n",
        "    return np.concatenate((real, imag))\n",
        "\n",
        "  def apply_gate(self, gate, inplace = False):\n",
        "    updated_amplitudes = gate.apply(self)\n",
        "    if inplace:\n",
        "      self._amplitudes = updated_amplitudes\n",
        "      return None\n",
        "\n",
        "    return QuantumState(list(updated_amplitudes.values()))\n",
        "\n",
        "  def fidelity_score(self, other):\n",
        "    # TODO: check on nielsen, implement well.\n",
        "    # This implementation is from paper\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      other (QuantumState): measure the fidelity between self and another quantum state\n",
        "    Return:\n",
        "      fidelity (float): fidelity score between [0, 1]\n",
        "    \"\"\"\n",
        "    # Inner product can be computed in terms of matrix representation. Page 67 Nielsen-Chuang\n",
        "    return np.square(abs(np.matmul(np.conj(self.get_amplitudes()), other.get_amplitudes())))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rboUEAK3ZbWt"
      },
      "source": [
        "### Quantum Gates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HOc4Ouiockr"
      },
      "source": [
        "class QuantumGate:\n",
        "  def __init__(self, name, unitary, target):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "      unitary: 2x2 unitary operator\n",
        "      target: 0 or 1 to denote the qubit the matrix is acting on\n",
        "    \"\"\"\n",
        "    self._name = name\n",
        "    self._U = unitary.flatten()\n",
        "    self._target = target\n",
        "\n",
        "\n",
        "  def apply(self, quantum_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      quantum_state (QuantumState): quantum state to which apply the quantum operator\n",
        "    Return:\n",
        "      updated (dict): didctionary with updated amplitudes\n",
        "    \"\"\"\n",
        "    # Directly implement update rule on the amplitudes for a 2 qubits case. \n",
        "    # NOTE: This approach is not scalable! Refine or use qiskit for more than 2 qubits\n",
        "    a_00, a_01, a_10, a_11 = quantum_state.get_amplitudes()\n",
        "    updated = dict()\n",
        "\n",
        "    if self._target == 0:\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_10\n",
        "      updated['01'] = self._U[0]*a_01 + self._U[1]*a_11\n",
        "      updated['10'] = self._U[2]*a_00 + self._U[3]*a_10\n",
        "      updated['11'] = self._U[2]*a_01 + self._U[3]*a_11\n",
        "\n",
        "    else:\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_01\n",
        "      updated['01'] = self._U[2]*a_00 + self._U[3]*a_01\n",
        "      updated['10'] = self._U[0]*a_10 + self._U[1]*a_11\n",
        "      updated['11'] = self._U[2]*a_10 + self._U[3]*a_11\n",
        "\n",
        "    # Check if amplitudes still satisfy condition\n",
        "    normalization = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, updated.values()))\n",
        "    error = f\"Error: sum of squared amplitudes must be = 1.\\n Amplitudes: {list(updated.values())}, summing up to {normalization}\"\n",
        "    assert math.isclose(normalization, 1, rel_tol = 1e-2), error\n",
        "    \n",
        "    return updated\n",
        "\n",
        "\n",
        "class CNOT(QuantumGate):\n",
        "  def __init__(self, control):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      control (int): if 0, first qubit is the control, else second\n",
        "    Return:\n",
        "      result (QuantumState): quantum state with amplitudes modified\n",
        "    \"\"\"\n",
        "    self._control = control\n",
        "    super().__init__('cnot', np.array([[0, 1], [1, 0]]), 1-control)\n",
        "\n",
        "\n",
        "  def apply(self, quantum_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      state (QuantumState): quantum state to which apply the quantum operator\n",
        "    Return:\n",
        "      result (QuantumState): quantum state with amplitudes modified\n",
        "    \"\"\"\n",
        "    # Directly implement update rule on the amplitudes for a 2 qubits case. \n",
        "    # NOTE: This approach is not scalable! Refine or use qiskit for more than 2 qubits\n",
        "    a_00, a_01, a_10, a_11 = quantum_state.get_amplitudes()\n",
        "    updated = dict()\n",
        "\n",
        "    if self._control == 0: # Then target = 2nd: if 1st qubit == 1, flip 2nd qubit.\n",
        "      updated['00'] = a_00\n",
        "      updated['01'] = a_01\n",
        "      updated['10'] = self._U[0]*a_10 + self._U[1]*a_11\n",
        "      updated['11'] = self._U[2]*a_10 + self._U[3]*a_11\n",
        "\n",
        "    if self._control == 1: # Then control = 1st: if 2nd qubit == 1, flip 1st qubit.\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_10\n",
        "      updated['01'] = self._U[0]*a_01 + self._U[1]*a_11\n",
        "      updated['10'] = a_10\n",
        "      updated['11'] = a_11\n",
        "\n",
        "    # Check if amplitudes still satisfy condition\n",
        "    normalization = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, updated.values()))\n",
        "    error = f\"Error: sum of squared amplitudes must be = 1.\\n Amplitudes: {list(updated.values())}, summing up to {normalization}\"\n",
        "    assert math.isclose(normalization, 1, rel_tol = 1e-2), error\n",
        "    \n",
        "    return updated"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfmGXb2TZWpw"
      },
      "source": [
        "### Gates List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pus5TovHiETW"
      },
      "source": [
        "# TODO: compare results with qiskit\n",
        "class Gates:\n",
        "  def __init__(self):\n",
        "    gates = dict()\n",
        "    self._num_gates = 0\n",
        "\n",
        "    # Useful\n",
        "    targets = [0, 1]\n",
        "    j = 1j # complex unit\n",
        "\n",
        "    ########### CNOT ###########\n",
        "    gates['CNOT'] = CNOT(control = 0)\n",
        "    self._num_gates += 1\n",
        "\n",
        "    ########### Rotations ###########\n",
        "    angles_names = ['pi', '2pi/3', 'pi/2', 'pi/3', 'pi/4']\n",
        "    angles_values = (math.pi / 2) * np.array([1, 2/3, 1/2, 1/3, 1/4])\n",
        "    angles = {k:v for k,v in zip(angles_names, angles_values)}\n",
        "\n",
        "    # Rx\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Rx' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[math.cos(theta), -j*math.sin(theta)],\n",
        "                                                      [-j*math.sin(theta), math.cos(theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "        \n",
        "    # Ry\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Ry' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[math.cos(theta), -math.sin(theta)],\n",
        "                                                      [math.sin(theta), math.cos(theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "\n",
        "    # Rz\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Rz' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[cmath.exp(-j*theta), 0],\n",
        "                                                      [0, cmath.exp(j*theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "\n",
        "    self._mapping = list(gates.keys())\n",
        "    self._gates = list(gates.values())\n",
        "\n",
        "  def num_gates(self):\n",
        "    return self._num_gates"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A11rxMP0zlf"
      },
      "source": [
        "# TODO: Compare simulation with qiskit results\n",
        "gates = Gates()\n",
        "q = QuantumState(np.array([0.4j, 0.3, 0.6, 0.624]))\n",
        "old_amplitudes = q.get_amplitudes()\n",
        "for g, name in zip(gates._gates, gates._mapping):\n",
        "  print(\"Gate \" + name)\n",
        "  q.apply_gate(g, inplace = True)\n",
        "  new_amplitudes = []\n",
        "  for val in q.get_amplitudes():\n",
        "    new_amplitudes.append(\"{:.3}\".format(val))\n",
        "\n",
        "  print(\"Applied gate \" + name + f\" to qubit with amplitudes {old_amplitudes}.\\n\" +\n",
        "        f\"Updated amplitudes: {new_amplitudes}\")\n",
        "  \n",
        "  print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQJvflImPuhP"
      },
      "source": [
        "# RL Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aflbZWf75E7C"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL6bfPeCJSbl"
      },
      "source": [
        "# LinearModel of the environment\n",
        "class LinearModel:\n",
        "  def __init__(self, initial_state, target_state, tolerance):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      initial_state (QuantumState): initial state of the system\n",
        "      target_state (QuantumState): target state of the system\n",
        "      tolerance (float): tolerance in terms of fidelity score\n",
        "    \"\"\"\n",
        "    self._initial_state = initial_state\n",
        "    self._target_state = target_state\n",
        "    self._tolerance = tolerance # tolerance in terms of fidelity between\n",
        "    self._quantum_gates, self._mapping, self._num_gates = self.gates_set()\n",
        "    self._terminal_fidelity = 0 # used to retrieve the info at the end of an episode\n",
        "    self._terminal_state = None # used to retrieve the info at the end of an episode\n",
        "    \n",
        "    # Used to initialize env from scratch.\n",
        "    self.reset() \n",
        "\n",
        "    assert initial_state.fidelity_score(target_state) < (1-tolerance), f\"The two state are the same up to {tolerance} tolerance\"\n",
        "\n",
        "    \n",
        "  def gates_set(self):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      gates._gates (List[QuantumGate]): return the list with available QuantumGate objects\n",
        "      gates._mapping (Dict[String: Int]): dicitonary mapping a gate to its action index\n",
        "      gates.num_gates() (int): number of available gates (actions)\n",
        "    \"\"\"\n",
        "    gates = Gates()\n",
        "    return gates._gates, gates._mapping, gates.num_gates()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Given a gate, apply it to self._state.\n",
        "    Set the reward, resulting state and discount.\n",
        "    Return these values, along with the information if therminal state has been reached\n",
        "    Args:\n",
        "      action (int): action index to select a QuantumGate\n",
        "\n",
        "    Return:\n",
        "      reward, discount, next_state features, terminal\n",
        "    \"\"\"\n",
        "    # Get and apply action. next_state is a QuantumState\n",
        "    gate = self._quantum_gates[action]\n",
        "    next_state = self._state.apply_gate(gate)\n",
        "\n",
        "    # Compare new state and target\n",
        "    fidelity = next_state.fidelity_score(self._target_state)\n",
        "\n",
        "    # Assign reward based on state and fidelity\n",
        "    terminal = self.is_terminal(fidelity)\n",
        "\n",
        "    # Terminal state\n",
        "    if terminal:\n",
        "      reward = +100.\n",
        "      discount = 0.\n",
        "      self._terminal_fidelity = fidelity\n",
        "      self._terminal_state = next_state\n",
        "      self.reset()\n",
        "    else:\n",
        "      reward = -6.\n",
        "      discount = 0.9\n",
        "      self._current_fidelity = fidelity\n",
        "      self._state = next_state\n",
        "\n",
        "\n",
        "    # Return the features, not the state itself\n",
        "    return reward, discount, self.get_obs(), terminal\n",
        "\n",
        "\n",
        "  def is_terminal(self, fidelity):\n",
        "    \"\"\"\n",
        "    Check if, by a level of self.tolerance, state is terminal\n",
        "    \"\"\"\n",
        "    if fidelity > (1 - self._tolerance):\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "  def get_obs(self):\n",
        "    return self._state.get_features()\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self._state = self._initial_state\n",
        "    self._current_fidelity = 0"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ZoAqcy3qkH"
      },
      "source": [
        "## Agent\n",
        "There is a bug I think, when I provide as next_state the initial state I\n",
        "- agent is in state self._state: a state contiguous to the terminal one, T, that we call S\n",
        "- the update is done for S wrt to next_state, which in this case is I, and not T as expected. There are 2 major drawbacks as consequence  \n",
        "One is that the update is done wrongly: r + g*q(I), but I here has not any sense\n",
        "Two is that I will never learn that this state is contiguous.\n",
        "\n",
        "Now, I have to reason about this, because I think in the assignment they used this approach, but better to write down this doubt.\n",
        "\n",
        "NO! This issue is fixed by putting discount = 0. Alright :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De-sHIvHwKra"
      },
      "source": [
        "# Least Square TD Agent: action value function approximation\n",
        "# implemented with gradient descent.\n",
        "class LSTDAgent:\n",
        "  def __init__(self, number_of_actions, number_of_features,\n",
        "      initial_state, step_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      initial_state: it is a feature vector!\n",
        "    \"\"\"\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._W = np.zeros((number_of_actions, number_of_features))\n",
        "    self._step_size = step_size\n",
        "    self._state = initial_state\n",
        "    self._action = random.choice(range(number_of_actions))\n",
        "\n",
        "  def behaviour_policy(self):\n",
        "    return random.choice(range(self._number_of_actions))\n",
        "\n",
        "  def q(self, state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      state (list): list of the amplitudes := features of the state\n",
        "    \"\"\"\n",
        "    # TODO: chiedere a davide per le features...\n",
        "    return np.matmul(self._W, state)\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      next_state (list): list of the amplitude: these are already features, not a QuantumState instance\n",
        "      terminal (boolean): if next_state is the terminal state\n",
        "    \"\"\"\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    next_s = next_state\n",
        "    g = discount\n",
        "\n",
        "    # WARNING: If s is complex, also the update become complex ... Can I decide to have complex weights?\n",
        "    self._W[a] += self._step_size * (r + g * np.max(self.q(next_s)) - self.q(s)[a]) * s\n",
        "\n",
        "    next_a = self.behaviour_policy()\n",
        "    self._action = next_a\n",
        "    self._state = next_s\n",
        "\n",
        "    return next_a\n",
        "\n",
        "  def inference(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    next_s = next_state\n",
        "    g = discount\n",
        "\n",
        "    next_a = np.argmax(self.q(next_s))\n",
        "    self._action = next_a\n",
        "    self._state = next_s\n",
        "\n",
        "    return next_a"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFL1yPnM06xH"
      },
      "source": [
        "# Experiment Facade and Helper Functions\n",
        "Class to handle experiments and visualization  \n",
        "- [X] run experiment\n",
        "- [ ] Allow for agent.q visualization for a state, providing labels to actions. This allow to see if very close states are associated to very close actions\n",
        "- [ ] Monitor number of steps as experience grow: is the agent actually learning good?\n",
        "- [ ] Monitor the weights (see if I can find any meaning): need to put labels about actions ==> does an action focuses on an amplitudes subset as I would expect (e.g. if an amplitude is not touched by  gate, than I expect its weight value to be low\n",
        "- [X] Monitor q value on 2D graphs.\n",
        "- [X] Monitor fidelity score inside episodes --> add inference every N steps\n",
        "- [ ] Monitor impact of initial gate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfMBR8Ws0sqy"
      },
      "source": [
        "# Facade class to run experiment\n",
        "class Experiment:\n",
        "  def __init__(self, start_state, target_state, tolerance, number_of_episodes, step_size, inference_ratio = 200):\n",
        "\n",
        "    self._env = LinearModel(QuantumState(start_state), QuantumState(target_state), tolerance)\n",
        "\n",
        "    number_of_actions = len(self._env._mapping)\n",
        "    initial_features = self._env.get_obs()\n",
        "\n",
        "    self._agent = LSTDAgent(number_of_actions, len(initial_features), initial_features,\n",
        "                            step_size)\n",
        "    \n",
        "    self._number_of_episodes = number_of_episodes\n",
        "\n",
        "    self._episodes_gates = [] # check disrtribution of gates in different solutions\n",
        "    self._mean_rewards = [] # list of mean reward for each episode\n",
        "    self._inference_ratio = inference_ratio # run an inference episode every inference_ratio training episodes\n",
        "\n",
        "\n",
        "  def run_experiment(self):\n",
        "    \"\"\"\n",
        "    Run episodes, gathering statistics and updating user on conosole.\n",
        "    \"\"\"\n",
        "    # Run inference on 1 out of inference_ratio training episodes\n",
        "    inference_gates = []\n",
        "    for episode in range(self._number_of_episodes):\n",
        "      self.run_episode()\n",
        "      if episode % self._inference_ratio == 0:\n",
        "        inference_gates.append(self.run_inference())\n",
        "\n",
        "    inference_gates.append(self.run_inference())\n",
        "\n",
        "    return inference_gates\n",
        "\n",
        "\n",
        "  def run_episode(self):\n",
        "    \"\"\"\n",
        "    Run a single episode.\n",
        "    At the beginning of an episode we must guarantee\n",
        "    - initial state in the environment\n",
        "    - initial state in the agent\n",
        "    \"\"\"\n",
        "    terminal = False\n",
        "    action = self._agent._action\n",
        "    while not terminal:\n",
        "      reward, discount, next_s, terminal = self._env.step(action)\n",
        "      action = self._agent.step(reward, discount, next_s)\n",
        "\n",
        "\n",
        "  def run_inference(self):\n",
        "    \"\"\"\n",
        "    Run an episode using the optimal policy learned\n",
        "    \"\"\"\n",
        "    terminal = False\n",
        "    action = self._agent._action\n",
        "    gates = [action]\n",
        "    while not terminal and len(gates) <= 1000:\n",
        "      reward, discount, next_s, terminal = self._env.step(action)\n",
        "      action = self._agent.inference(reward, discount, next_s)\n",
        "      gates.append(action)\n",
        "\n",
        "    if len(gates) >= 100:\n",
        "      print(f\"Couldn't reach target in less than 100 steps. Current fidelity {self._env._current_fidelity}.\")\n",
        "      return []\n",
        "\n",
        "    self._agent._step_size *= .1\n",
        "    sns.heatmap(self._agent._W)\n",
        "    plt.show()\n",
        "    print(f\"Inference completed in {len(gates)} steps. Fidelity score: {self._env._terminal_fidelity}\")\n",
        "\n",
        "    return gates\n",
        "\n",
        "  \n",
        "  def q_values(self):\n",
        "    \"\"\"\n",
        "    The function should plot, in a [0, 1] complex plane, the value function\n",
        "    on the z axis, for each of the 4 value of amplitude.\n",
        "    In order to get reasonable values and understand the effect of the amplitude\n",
        "    under analysis on the overall q_values, the remaining amplitudes values will \n",
        "    be set to the target amplitudes.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "\n",
        "    state = self._env._target_state\n",
        "    features = state.get_features()\n",
        "    x = np.linspace(0, 1, num=100)\n",
        "    y = np.linspace(0, 1, num=100)\n",
        "    grid = np.meshgrid\n",
        "    q = []\n",
        "    for i in x:\n",
        "      row = []\n",
        "      for k in y:\n",
        "        features[0] = i\n",
        "        features[0] = k\n",
        "        row.append(self._agent.q(features))\n",
        "      \n",
        "      q.append(row)\n",
        "\n",
        "    q = np.array(q)\n",
        "    surf = ax.plot_surface(x, y, np.squeeze(q[:, :, 0]), cmap=cm.coolwarm,\n",
        "                       linewidth=0, antialiased=False)\n",
        "    \n",
        "    # Add a color bar which maps values to colors.\n",
        "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YKa4MYQ0tUp"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN0SmXxG_LPx"
      },
      "source": [
        "- Need to tune all the parameters below. In particular, it is important to understand upt to which point we can decrease the tolerance.\n",
        "- Substitute TRAINING_EPISODES with number of training steps "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R5nEorB3kgW"
      },
      "source": [
        "TOLERANCE = 0.2\n",
        "TRAINING_EPISODES = 5000\n",
        "STEP_SIZE = 0.0001\n",
        "EPS = 0.8"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW1bekJrCTYu"
      },
      "source": [
        "# Run an inference every N episodes and monitor the optimality of the behaviour\n",
        "# We could add experience replay, since after a certain number of new episodes it doesn't work well anymore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b2pDqPiVFxp",
        "outputId": "05deaecd-cc5b-43a3-aff5-375853bdbb93"
      },
      "source": [
        "start = [0.1j, 0.1, 0.1, -0.985]\n",
        "target = [0.5, 0.5, 0.5, 0.5]\n",
        "experiment = Experiment(start, target, TOLERANCE, TRAINING_EPISODES, STEP_SIZE)\n",
        "gates_sequences = experiment.run_experiment()\n",
        "len_sequences = list(map(lambda x: len(x), gates_sequences))\n",
        "print()\n",
        "print(experiment._agent._step_size)\n",
        "sns.heatmap(experiment._agent._W)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Couldn't reach target in less than 100 steps. Current fidelity 0.07588468802638092.\n",
            "Couldn't reach target in less than 100 steps. Current fidelity 0.4572811878561952.\n",
            "Inference completed in 8 steps. Fidelity score: 0.8631647787132828\n",
            "Couldn't reach target in less than 100 steps. Current fidelity 0.03301325433846086.\n",
            "Inference completed in 10 steps. Fidelity score: 0.8323788234168618\n",
            "Inference completed in 10 steps. Fidelity score: 0.8058943563079755\n",
            "Inference completed in 7 steps. Fidelity score: 0.9551521216744563\n",
            "Inference completed in 12 steps. Fidelity score: 0.8346593728098757\n",
            "Inference completed in 5 steps. Fidelity score: 0.844638059174456\n",
            "Inference completed in 5 steps. Fidelity score: 0.844638059174456\n",
            "Inference completed in 7 steps. Fidelity score: 0.9551521216744563\n",
            "Inference completed in 5 steps. Fidelity score: 0.844638059174456\n",
            "Couldn't reach target in less than 100 steps. Current fidelity 0.16047046901320886.\n",
            "Inference completed in 12 steps. Fidelity score: 0.8346593728098757\n",
            "Inference completed in 9 steps. Fidelity score: 0.8346593728098757\n",
            "Couldn't reach target in less than 100 steps. Current fidelity 0.17158163021678585.\n",
            "Couldn't reach target in less than 100 steps. Current fidelity 0.16047046901320886.\n",
            "Couldn't reach target in less than 100 steps. Current fidelity 0.5127993728959325.\n",
            "Couldn't reach target in less than 100 steps. Current fidelity 0.265056250000016.\n",
            "Couldn't reach target in less than 100 steps. Current fidelity 0.5127993728959325.\n",
            "Inference completed in 8 steps. Fidelity score: 0.8464681861933783\n",
            "Couldn't reach target in less than 100 steps. Current fidelity 0.17158163021678585.\n",
            "Inference completed in 11 steps. Fidelity score: 0.8036796068537995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "vnql3KhQOUS7",
        "outputId": "08104b8e-b4c7-4010-ec3c-958c74be254c"
      },
      "source": [
        ""
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff37f9d8b50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5-DR4p_QJ8o",
        "outputId": "67c0d05c-9cf9-40e9-ed24-d6d0325384a8"
      },
      "source": [
        "len(res[0])"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy2c4fkW4QCC",
        "outputId": "bef57749-c973-4f16-f9fd-fef4ac2f59e5"
      },
      "source": [
        "experiment._env._terminal_state.get_amplitudes()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.70876626+0.01913417j, -0.38731851-0.04619398j,\n",
              "       -0.51588681-0.03314136j, -0.26780692+0.08001031j])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pss1hqFz3G_",
        "outputId": "cb708b77-5127-497e-d7f7-41595de71b87"
      },
      "source": [
        "experiment._env._terminal_state.fidelity_score(experiment._env._target_state)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.856652121674456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0Tz6BuBybfY",
        "outputId": "6efea5e7-16c1-46f6-f54d-3730158abb5e"
      },
      "source": [
        "len(experiment.run_inference()[0])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "380"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}