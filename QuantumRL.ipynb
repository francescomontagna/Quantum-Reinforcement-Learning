{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuantumRL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6TpA3WZa4ZxHb8tjKs0oe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescomontagna/Quantum-Reinforcement-Learning/blob/main/QuantumRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ywYaaLPmtL"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETGJGzcDIxwC"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import cmath\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from functools import reduce"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWCQSEmAOUI3"
      },
      "source": [
        "# Introduction\n",
        "## Problems\n",
        "- Since we can access the features, and know the transition probability from a state to another, this RL protocol is model based.\n",
        "Note that this is in contrast with the uncertainty about a quantum state from the observator point of view: an observator can only access the collapsed state, having no access to the amplitudes. \n",
        "\n",
        "Can I mentally assume the agent to be the system under study itself? Or this is completely unreasonale? \n",
        "\n",
        "In the paper Girolami sent me, they explicitly account for this fact, setting up a model free protocol.  \n",
        "\n",
        "- With features defined as amplitude (at the moment looks the only one reasonable, but has model-based problem + the one I am introducing) we have the problem that `np.matmul(self._W, state)` is an array of complex.  \n",
        "How can I do `np.argmax(complex_array)`? I can not.  \n",
        "One possible solution is to use the absolute over each element, but I am not sure...\n",
        "\n",
        "\n",
        "## Reward\n",
        "In Girolami's paper the used\n",
        "\n",
        "\\begin{equation}\n",
        "r(t) = \\begin{cases}\n",
        "0, \\;\\; if \\;\\;t < T\\\\\n",
        "F(T) = |<\\psi^*| |\\psi>| , \\;\\; if \\;\\;t = T\\\\\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "This notion is not applicable to my case to determine the terminal state, since I do not have a time stape T associated to it. But the use of fidelity is justified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-LCGYpUPpTm"
      },
      "source": [
        "# Quantum Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV8sEgIgZisp"
      },
      "source": [
        "### Qubit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjboXlFWG5EK"
      },
      "source": [
        "class Qubit:\n",
        "  def __init__(self, amplitudes):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      amplitudes (np.array): amplitudes of the |0>, |1> vectors\n",
        "    \"\"\"\n",
        "    error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "    assert math.isclose(reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes)), 1, rel_tol = 1e-2), error\n",
        "    self._amplitudes = amplitudes\n",
        "\n",
        "  def qubit(self):\n",
        "    return self._amplitudes\n",
        "    \n",
        "\n",
        "class Basis(Qubit):\n",
        "  def __init__(self, index):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      index: index of the position = 1\n",
        "    \"\"\"\n",
        "    amplitudes = np.zeros((2, ))\n",
        "    amplitudes[index] += 1\n",
        "    super().__init__(amplitudes)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnWqxby5ZfY_"
      },
      "source": [
        "### Quantum State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRBaWNuZyhgp"
      },
      "source": [
        "class QuantumState:\n",
        "  def __init__(self, amplitudes): \n",
        "\n",
        "    # We assume computational basis\n",
        "    self._basis = {\n",
        "        '00': [Basis(0), Basis(0)],\n",
        "        '01': [Basis(0), Basis(1)],\n",
        "        '10': [Basis(1), Basis(0)],\n",
        "        '11': [Basis(1), Basis(1)],\n",
        "    }\n",
        "\n",
        "    a1, a2, a3, a4 = amplitudes\n",
        "    self._amplitudes = {\n",
        "        '00': a1,\n",
        "        '01': a2,\n",
        "        '10': a3,\n",
        "        '11': a4\n",
        "    }\n",
        "    \n",
        "    self._keys = list(self._basis.keys())\n",
        "\n",
        "    error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "    val = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes))\n",
        "    assert math.isclose(val, 1, rel_tol = 1e-2), error + f\" instead is {val}\"\n",
        "\n",
        "  def get_amplitudes(self):\n",
        "    return np.array(list(self._amplitudes.values()))\n",
        "\n",
        "  def get_features(self):\n",
        "    real = np.real(list(self._amplitudes.values()))\n",
        "    imag = np.imag(list(self._amplitudes.values()))\n",
        "    return np.concatenate((real, imag))\n",
        "\n",
        "  def apply_gate(self, gate, inplace = False):\n",
        "    updated_amplitudes = gate.apply(self)\n",
        "    if inplace:\n",
        "      self._amplitudes = updated_amplitudes\n",
        "      return None\n",
        "\n",
        "    return QuantumState(list(updated_amplitudes.values()))\n",
        "\n",
        "  def fidelity_score(self, other):\n",
        "    # TODO: check on nielsen, implement well.\n",
        "    # This implementation is from paper\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      other (QuantumState): measure the fidelity between self and another quantum state\n",
        "    Return:\n",
        "      fidelity (float): fidelity score between [0, 1]\n",
        "    \"\"\"\n",
        "    # Inner product can be computed in terms of matrix representation. Page 67 Nielsen-Chuang\n",
        "    return np.square(abs(np.matmul(np.conj(self.get_amplitudes()), other.get_amplitudes())))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rboUEAK3ZbWt"
      },
      "source": [
        "### Quantum Gates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HOc4Ouiockr"
      },
      "source": [
        "class QuantumGate:\n",
        "  def __init__(self, name, unitary, target):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "      unitary: 2x2 unitary operator\n",
        "      target: 0 or 1 to denote the qubit the matrix is acting on\n",
        "    \"\"\"\n",
        "    self._name = name\n",
        "    self._U = unitary.flatten()\n",
        "    self._target = target\n",
        "\n",
        "\n",
        "  def apply(self, quantum_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      quantum_state (QuantumState): quantum state to which apply the quantum operator\n",
        "    Return:\n",
        "      updated (dict): didctionary with updated amplitudes\n",
        "    \"\"\"\n",
        "    # Directly implement update rule on the amplitudes for a 2 qubits case. \n",
        "    # NOTE: This approach is not scalable! Refine or use qiskit for more than 2 qubits\n",
        "    a_00, a_01, a_10, a_11 = quantum_state.get_amplitudes()\n",
        "    updated = dict()\n",
        "\n",
        "    if self._target == 0:\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_10\n",
        "      updated['01'] = self._U[0]*a_01 + self._U[1]*a_11\n",
        "      updated['10'] = self._U[2]*a_00 + self._U[3]*a_10\n",
        "      updated['11'] = self._U[2]*a_01 + self._U[3]*a_11\n",
        "\n",
        "    else:\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_01\n",
        "      updated['01'] = self._U[2]*a_00 + self._U[3]*a_01\n",
        "      updated['10'] = self._U[0]*a_10 + self._U[1]*a_11\n",
        "      updated['11'] = self._U[2]*a_10 + self._U[3]*a_11\n",
        "\n",
        "    # Check if amplitudes still satisfy condition\n",
        "    normalization = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, updated.values()))\n",
        "    error = f\"Error: sum of squared amplitudes must be = 1.\\n Amplitudes: {list(updated.values())}, summing up to {normalization}\"\n",
        "    assert math.isclose(normalization, 1, rel_tol = 1e-2), error\n",
        "    \n",
        "    return updated\n",
        "\n",
        "\n",
        "class CNOT(QuantumGate):\n",
        "  def __init__(self, control):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      control (int): if 0, first qubit is the control, else second\n",
        "    Return:\n",
        "      result (QuantumState): quantum state with amplitudes modified\n",
        "    \"\"\"\n",
        "    self._control = control\n",
        "    super().__init__('cnot', np.array([[0, 1], [1, 0]]), 1-control)\n",
        "\n",
        "\n",
        "  def apply(self, quantum_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      state (QuantumState): quantum state to which apply the quantum operator\n",
        "    Return:\n",
        "      result (QuantumState): quantum state with amplitudes modified\n",
        "    \"\"\"\n",
        "    # Directly implement update rule on the amplitudes for a 2 qubits case. \n",
        "    # NOTE: This approach is not scalable! Refine or use qiskit for more than 2 qubits\n",
        "    a_00, a_01, a_10, a_11 = quantum_state.get_amplitudes()\n",
        "    updated = dict()\n",
        "\n",
        "    if self._control == 0: # Then target = 2nd: if 1st qubit == 1, flip 2nd qubit.\n",
        "      updated['00'] = a_00\n",
        "      updated['01'] = a_01\n",
        "      updated['10'] = self._U[0]*a_10 + self._U[1]*a_11\n",
        "      updated['11'] = self._U[2]*a_10 + self._U[3]*a_11\n",
        "\n",
        "    if self._control == 1: # Then control = 1st: if 2nd qubit == 1, flip 1st qubit.\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_10\n",
        "      updated['01'] = self._U[0]*a_01 + self._U[1]*a_11\n",
        "      updated['10'] = a_10\n",
        "      updated['11'] = a_11\n",
        "\n",
        "    # Check if amplitudes still satisfy condition\n",
        "    normalization = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, updated.values()))\n",
        "    error = f\"Error: sum of squared amplitudes must be = 1.\\n Amplitudes: {list(updated.values())}, summing up to {normalization}\"\n",
        "    assert math.isclose(normalization, 1, rel_tol = 1e-2), error\n",
        "    \n",
        "    return updated"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfmGXb2TZWpw"
      },
      "source": [
        "### Gates List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pus5TovHiETW"
      },
      "source": [
        "# Why don't I implement universal gates only?\n",
        "# TODO: compare results with qiskit\n",
        "class Gates:\n",
        "  def __init__(self):\n",
        "    gates = dict()\n",
        "    self._num_gates = 0\n",
        "\n",
        "    # Useful\n",
        "    targets = [0, 1]\n",
        "    j = 1j # complex unit\n",
        "\n",
        "    ########### CNOT ###########\n",
        "    gates['CNOT'] = CNOT(control = 0)\n",
        "    self._num_gates += 1\n",
        "\n",
        "    ########### Rotations ###########\n",
        "    angles_names = ['pi', '2pi/3', 'pi/2', 'pi/3', 'pi/4']\n",
        "    angles_values = (math.pi / 2) * np.array([1, 2/3, 1/2, 1/3, 1/4])\n",
        "    angles = {k:v for k,v in zip(angles_names, angles_values)}\n",
        "\n",
        "    # X\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Rx' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[math.cos(theta), -j*math.sin(theta)],\n",
        "                                                      [-j*math.sin(theta), math.cos(theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "        \n",
        "    # Y\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Ry' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[math.cos(theta), -math.sin(theta)],\n",
        "                                                      [math.sin(theta), math.cos(theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "\n",
        "    # Z\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Rz' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[cmath.exp(-j*theta), 0],\n",
        "                                                      [0, cmath.exp(j*theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "\n",
        "    self._gates_list = list(gates.keys())\n",
        "    self._gates = list(gates.values())\n",
        "\n",
        "  def num_gates(self):\n",
        "    return self._num_gates"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A11rxMP0zlf"
      },
      "source": [
        "gates = Gates()\n",
        "\n",
        "# Make several simulations and compare results with qiskit\n",
        "q = QuantumState(np.array([0.4j, 0.3, 0.6, 0.624]))\n",
        "old_amplitudes = q.get_amplitudes()\n",
        "for g, name in zip(gates._gates, gates._gates_list):\n",
        "  print(\"Gate \" + name)\n",
        "  q.apply_gate(g, inplace = True)\n",
        "  new_amplitudes = []\n",
        "  for val in q.get_amplitudes():\n",
        "    new_amplitudes.append(\"{:.3}\".format(val))\n",
        "\n",
        "  print(\"Applied gate \" + name + f\" to qubit with amplitudes {old_amplitudes}.\\n\" +\n",
        "        f\"Updated amplitudes: {new_amplitudes}\")\n",
        "  \n",
        "  print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQJvflImPuhP"
      },
      "source": [
        "# RL Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aflbZWf75E7C"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL6bfPeCJSbl"
      },
      "source": [
        "# LinearModel of the environment\n",
        "class LinearModel:\n",
        "  def __init__(self, initial_state, target_state, tolerance):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      initial_state (QuantumState): initial state of the system\n",
        "      target_state (QuantumState): target state of the system\n",
        "      tolerance (float): tolerance in terms of fidelity score\n",
        "    \"\"\"\n",
        "    self._initial_state = initial_state\n",
        "    self._target_state = target_state\n",
        "    self._tolerance = tolerance # tolerance in terms of fidelity between\n",
        "    self._quantum_gates, self._gates_list, self._num_gates = self.gates_set()\n",
        "    self._terminal_fidelity = 0 # used to retrieve the info at the end of an episode\n",
        "    self._terminal_state = initial_state # used to retrieve the info at the end of an episode\n",
        "    self._episode_length = 10\n",
        "    \n",
        "    # Used to initialize env from scratch.\n",
        "    self.reset() \n",
        "\n",
        "    assert initial_state.fidelity_score(target_state) < (1-tolerance), f\"The two state are the same up to {tolerance} tolerance\"\n",
        "\n",
        "    \n",
        "  def gates_set(self):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      gates._gates (List[QuantumGate]): return the list with available QuantumGate objects\n",
        "      gates._gates_list (List[String]): list with the name of available gates\n",
        "    \"\"\"\n",
        "    gates = Gates()\n",
        "    return gates._gates, gates._gates_list, gates.num_gates()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Given a gate, apply it to self._state.\n",
        "    Set the reward, resulting state and discount.\n",
        "    Return these values, along with the information if therminal state has been reached\n",
        "    Args:\n",
        "      action (int): action index to select a QuantumGate\n",
        "\n",
        "    Return:\n",
        "      reward, discount, next_state features, terminal\n",
        "    \"\"\"\n",
        "    # Get and apply action. next_state is a QuantumState\n",
        "    gate = self._quantum_gates[action]\n",
        "    next_state = self._state.apply_gate(gate)\n",
        "\n",
        "    # Compare new state and target\n",
        "    fidelity = next_state.fidelity_score(self._target_state)\n",
        "\n",
        "    # Assign reward based on state and fidelity\n",
        "    terminal = self.is_terminal(fidelity)\n",
        "\n",
        "    # Terminal state\n",
        "    if terminal:\n",
        "      reward = +100.\n",
        "      discount = 0.\n",
        "      self._terminal_fidelity = fidelity\n",
        "      self._terminal_state = next_state\n",
        "      self.reset()\n",
        "    else:\n",
        "      reward = -1\n",
        "      discount = 0.9\n",
        "      self._current_fidelity = fidelity\n",
        "      self._state = next_state\n",
        "\n",
        "    # # Terminal state\n",
        "    # terminal = self._episode_counter == self._episode_length or self.is_terminal(fidelity)\n",
        "    # if terminal:\n",
        "    #   if self.is_terminal(fidelity):\n",
        "    #     reward = 1\n",
        "    #   else:\n",
        "    #     reward = 0\n",
        "    #   discount = 0.\n",
        "    #   self._terminal_fidelity = fidelity\n",
        "    #   self._terminal_state = next_state\n",
        "    #   self.reset()\n",
        "\n",
        "    # else:\n",
        "    #   reward = 0\n",
        "    #   discount = 1\n",
        "    #   self._current_fidelity = fidelity\n",
        "    #   self._state = next_state\n",
        "    #   self._episode_counter += 1\n",
        "\n",
        "    # Return the features, not the state itself\n",
        "    return reward, discount, self.get_obs(), terminal\n",
        "\n",
        "\n",
        "  def is_terminal(self, fidelity):\n",
        "    \"\"\"\n",
        "    Check if, by a level of self.tolerance, state is terminal\n",
        "    \"\"\"\n",
        "    if fidelity > (1 - self._tolerance):\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "  def get_obs(self):\n",
        "    return self._state.get_features()\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self._episode_counter = 0\n",
        "    self._state = self._initial_state\n",
        "    self._current_fidelity = 0"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ZoAqcy3qkH"
      },
      "source": [
        "## Agent\n",
        "There is a bug I think, when I provide as next_state the initial state I\n",
        "- agent is in state self._state: a state contiguous to the terminal one, T, that we call S\n",
        "- the update is done for S wrt to next_state, which in this case is I, and not T as expected. There are 2 major drawbacks as consequence  \n",
        "One is that the update is done wrongly: r + g*q(I), but I here has not any sense\n",
        "Two is that I will never learn that this state is contiguous.\n",
        "\n",
        "Now, I have to reason about this, because I think in the assignment they used this approach, but better to write down this doubt.\n",
        "\n",
        "NO! This issue is fixed by putting discount = 0. Alright :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De-sHIvHwKra"
      },
      "source": [
        "# Least Square TD Agent: action value function approximation\n",
        "# implemented with gradient descent.\n",
        "class LSTDAgent:\n",
        "  def __init__(self, number_of_actions, number_of_features,\n",
        "      initial_state, step_size, eps):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      initial_state: it is a feature vector!\n",
        "    \"\"\"\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._W = np.zeros((number_of_actions, number_of_features), dtype = 'complex128')\n",
        "    self._step_size = step_size\n",
        "    self._eps = eps\n",
        "    self._state = initial_state\n",
        "    self._action = random.choice(range(number_of_actions))\n",
        "\n",
        "  def behaviour_policy(self):\n",
        "    return random.choice(range(self._number_of_actions))\n",
        "\n",
        "  # def eps_greedy(self, state):\n",
        "  #   \"\"\"\n",
        "  #   Args:\n",
        "  #     state (list): list of the amplitudes := features of the state\n",
        "  #   \"\"\"\n",
        "  #   greedy = np.random.choice([True, False], p = (1-self._eps, self._eps))\n",
        "  #   if greedy:\n",
        "  #     return np.argmax(self.q(state))\n",
        "\n",
        "  #   return random.choice(range(self._number_of_actions))\n",
        "\n",
        "  def q(self, state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      state (list): list of the amplitudes := features of the state\n",
        "    \"\"\"\n",
        "    # TODO: chiedere a davide per le features...\n",
        "    return np.matmul(self._W, state)\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      next_state (list): list of the amplitude: these are already features, not a QuantumState instance\n",
        "      terminal (boolean): if next_state is the terminal state\n",
        "    \"\"\"\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    next_s = next_state\n",
        "    g = discount\n",
        "\n",
        "    # WARNING: If s is complex, also the update become complex ... Can I decide to have complex weights?\n",
        "    self._W[a] += self._step_size * (r + g * np.max(self.q(next_s)) - self.q(s)[a]) * s\n",
        "\n",
        "    next_a = self.behaviour_policy()\n",
        "    self._action = next_a\n",
        "    self._state = next_s\n",
        "\n",
        "    return next_a\n",
        "\n",
        "  def inference(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    next_s = next_state\n",
        "    g = discount\n",
        "\n",
        "    next_a = np.argmax(self.q(next_s))\n",
        "    self._action = next_a\n",
        "    self._state = next_s\n",
        "\n",
        "    return next_a"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFL1yPnM06xH"
      },
      "source": [
        "# Helper Functions\n",
        "Class to handle experiments and visualization\n",
        "- run experiment\n",
        "- Allow for agent.q visualization for a state, providing labels to actions. This allow to see if very close states are associated to very close actions\n",
        "- Monitor number of steps as experience grow: is the agent actually learning good?\n",
        "- Monitor the weights (see if I can find any meaning): need to put labels about actions ==> does an action focuses on an amplitudes subset as I would expect (e.g. if an amplitude is not touched by  gate, than I expect its weight value to be low\n",
        "- Monitor q value on 2D graphs (choose a subset of amplitudes).\n",
        "- Monitor fidelity score inside episodes\n",
        "- Monitor impact of initial gate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfMBR8Ws0sqy"
      },
      "source": [
        "# Facade\n",
        "class Experiment:\n",
        "  def __init__(self, start_state, target_state, tolerance, number_of_episodes, step_size, eps, verbosity = 0.1):\n",
        "\n",
        "    self._env = LinearModel(QuantumState(start_state), QuantumState(target_state), tolerance)\n",
        "\n",
        "    number_of_actions = len(self._env._gates_list)\n",
        "    initial_features = self._env.get_obs()\n",
        "\n",
        "    self._agent = LSTDAgent(number_of_actions, len(initial_features), initial_features,\n",
        "                            step_size, eps)\n",
        "    \n",
        "    self._number_of_episodes = number_of_episodes\n",
        "\n",
        "    self._episodes_gates = [] # check disrtribution of gates in different solutions\n",
        "    self._mean_rewards = [] # list of mean reward for each episode\n",
        "\n",
        "    self._verbosity = verbosity\n",
        "\n",
        "\n",
        "  def run_experiment(self):\n",
        "    \"\"\"\n",
        "    Run episodes, gathering statistics and updating user on conosole.\n",
        "    \"\"\"\n",
        "    for episode in range(self._number_of_episodes):\n",
        "      gates, mean_reward = self.run_episode()\n",
        "      if (np.random.choice([True, False], p = [self._verbosity ,1-self._verbosity])):\n",
        "        print(f\"Episode {episode}: completed in {len(gates)} steps. Fidelity score: {self._env._terminal_fidelity}\")\n",
        "      self._mean_rewards.append(mean_reward)\n",
        "      self._episodes_gates.append(gates)\n",
        "\n",
        "    self._agent._eps = 0\n",
        "    gates, mean_reward = self.run_inference()\n",
        "    print(f\"Inference completed in {len(gates)} steps. Fidelity score: {self._env._terminal_fidelity}\")\n",
        "    self._mean_rewards.append(mean_reward)\n",
        "    self._episodes_gates.append(gates)\n",
        "\n",
        "    return self._mean_rewards, self._episodes_gates\n",
        "\n",
        "  def run_episode(self):\n",
        "    \"\"\"\n",
        "    Run a single episode.\n",
        "    At the beginning of an episode we must guarantee\n",
        "    - initial state in the environment\n",
        "    - initial state in the agent\n",
        "    \"\"\"\n",
        "    terminal = False\n",
        "    action = self._agent._action\n",
        "    gates = [action]\n",
        "    mean_reward = 0\n",
        "    i = 0\n",
        "    while not terminal:\n",
        "      reward, discount, next_s, terminal = self._env.step(action)\n",
        "      action = self._agent.step(reward, discount, next_s)\n",
        "\n",
        "      i += 1\n",
        "      mean_reward += (reward - mean_reward)/i\n",
        "      gates.append(action)\n",
        "\n",
        "    return gates, mean_reward\n",
        "\n",
        "  def run_inference(self):\n",
        "    \"\"\"\n",
        "    Run a single episode.\n",
        "    At the beginning of an episode we must guarantee\n",
        "    - initial state in the environment\n",
        "    - initial state in the agent\n",
        "    \"\"\"\n",
        "    terminal = False\n",
        "    action = self._agent._action\n",
        "    gates = [action]\n",
        "    mean_reward = 0\n",
        "    i = 0\n",
        "    while not terminal:\n",
        "      reward, discount, next_s, terminal = self._env.step(action)\n",
        "      action = self._agent.inference(reward, discount, next_s)\n",
        "\n",
        "      i += 1\n",
        "      mean_reward += (reward - mean_reward)/i\n",
        "      gates.append(action)\n",
        "\n",
        "    return gates, mean_reward"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YKa4MYQ0tUp"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R5nEorB3kgW"
      },
      "source": [
        "TOLERANCE = 0.2\n",
        "NUM_EPISODES = 5000\n",
        "STEP_SIZE = 0.005\n",
        "EPS = .5"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-HiekQ5ofpS"
      },
      "source": [
        "gates_map = {k:i for i, k in enumerate(experiment._env._gates_list)}"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b2pDqPiVFxp",
        "outputId": "b370e1da-fbdf-475b-aea1-f994ae37cb1a"
      },
      "source": [
        "start = [0.1j, 0.1, 0.1, -0.985]\n",
        "target = [0.5, 0.5, 0.5, 0.5]\n",
        "experiment = Experiment(start, target, TOLERANCE, NUM_EPISODES, STEP_SIZE, EPS, verbosity = 0.01)\n",
        "mean_rewards, gates_sequences = experiment.run_experiment()\n",
        "len_sequences = list(map(lambda x: len(x), gates_sequences))\n",
        "sns.heatmap(np.absolute(experiment._agent._W))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 5: completed in 26 steps. Fidelity score: 0.837743292570598\n",
            "Episode 82: completed in 85 steps. Fidelity score: 0.8715501076198633\n",
            "Episode 151: completed in 55 steps. Fidelity score: 0.808549504559307\n",
            "Episode 240: completed in 36 steps. Fidelity score: 0.861360924973243\n",
            "Episode 304: completed in 63 steps. Fidelity score: 0.9338165160698179\n",
            "Episode 344: completed in 131 steps. Fidelity score: 0.8832741685480683\n",
            "Episode 583: completed in 71 steps. Fidelity score: 0.919082428669965\n",
            "Episode 682: completed in 106 steps. Fidelity score: 0.8387179711227765\n",
            "Episode 736: completed in 102 steps. Fidelity score: 0.8995492857161441\n",
            "Episode 944: completed in 19 steps. Fidelity score: 0.8067547454359335\n",
            "Episode 1092: completed in 719 steps. Fidelity score: 0.8628675471234449\n",
            "Episode 1099: completed in 708 steps. Fidelity score: 0.8858779426347638\n",
            "Episode 1280: completed in 558 steps. Fidelity score: 0.9583127246558596\n",
            "Episode 1308: completed in 9 steps. Fidelity score: 0.9606096212014563\n",
            "Episode 1643: completed in 188 steps. Fidelity score: 0.8663798529737113\n",
            "Episode 1891: completed in 220 steps. Fidelity score: 0.8707538274491998\n",
            "Episode 1995: completed in 437 steps. Fidelity score: 0.822963063859215\n",
            "Episode 2002: completed in 182 steps. Fidelity score: 0.8482585607817038\n",
            "Episode 2028: completed in 107 steps. Fidelity score: 0.87176556846635\n",
            "Episode 2121: completed in 512 steps. Fidelity score: 0.8566839698156573\n",
            "Episode 2151: completed in 259 steps. Fidelity score: 0.9049810370400019\n",
            "Episode 2295: completed in 191 steps. Fidelity score: 0.8093070420193641\n",
            "Episode 2321: completed in 229 steps. Fidelity score: 0.8355056782770819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_tOBSEki3Bs",
        "outputId": "f24a8a67-c5d2-416e-cac1-424c99ee13fa"
      },
      "source": [
        "env._gates_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CNOT',\n",
              " 'Rx0(pi)',\n",
              " 'Rx1(pi)',\n",
              " 'Rx0(2pi/3)',\n",
              " 'Rx1(2pi/3)',\n",
              " 'Rx0(pi/2)',\n",
              " 'Rx1(pi/2)',\n",
              " 'Rx0(pi/3)',\n",
              " 'Rx1(pi/3)',\n",
              " 'Rx0(pi/4)',\n",
              " 'Rx1(pi/4)',\n",
              " 'Ry0(pi)',\n",
              " 'Ry1(pi)',\n",
              " 'Ry0(2pi/3)',\n",
              " 'Ry1(2pi/3)',\n",
              " 'Ry0(pi/2)',\n",
              " 'Ry1(pi/2)',\n",
              " 'Ry0(pi/3)',\n",
              " 'Ry1(pi/3)',\n",
              " 'Ry0(pi/4)',\n",
              " 'Ry1(pi/4)',\n",
              " 'Rz0(pi)',\n",
              " 'Rz1(pi)',\n",
              " 'Rz0(2pi/3)',\n",
              " 'Rz1(2pi/3)',\n",
              " 'Rz0(pi/2)',\n",
              " 'Rz1(pi/2)',\n",
              " 'Rz0(pi/3)',\n",
              " 'Rz1(pi/3)',\n",
              " 'Rz0(pi/4)',\n",
              " 'Rz1(pi/4)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 518
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWGqX2Dqg9BI",
        "outputId": "e2b701b9-fd2c-4a35-ab7f-3d9daba0db7e"
      },
      "source": [
        "for el in gates_sequences[93]:\n",
        "  print(experiment._env._gates_list[el])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ry0(pi)\n",
            "Ry0(pi)\n",
            "Ry0(pi)\n",
            "Rx1(pi)\n",
            "Ry0(pi)\n",
            "Ry0(pi/4)\n",
            "Ry1(pi/2)\n",
            "Ry0(pi)\n",
            "Ry1(pi/4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zhmFchNfW63",
        "outputId": "d9acf289-9fdb-4004-ec9e-115249ef9278"
      },
      "source": [
        "smaller = []\n",
        "larger = []\n",
        "for seq in gates_sequences:\n",
        "  for el in seq:\n",
        "    if el < 11:\n",
        "      smaller.append(el)\n",
        "    else:\n",
        "      larger.append(el)\n",
        "\n",
        "print(len(smaller))\n",
        "print(len(larger))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14000\n",
            "25646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icrtlDvd7pzN",
        "outputId": "c0807c53-2bee-4e78-9d0b-82942914b789"
      },
      "source": [
        "experiment._agent.q([ 0.+0.1j,  0.1+0.j ,-0.985+0.j ,0.1  +0.j ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00289628, 0.00087334, 0.00102635, 0.00080123, 0.0032333 ,\n",
              "       0.00052766, 0.00152014, 0.00152272, 0.00204501, 0.00127306,\n",
              "       0.00111846, 0.22403655, 0.13441044, 0.23439259, 0.1992544 ,\n",
              "       0.03835872, 0.08865378, 0.10928882, 0.13321601, 0.08527924,\n",
              "       0.19770758, 0.04134601, 0.21037818, 0.11676777, 0.05268605,\n",
              "       0.09026159, 0.18695474, 0.13446268, 0.05108042, 0.08269479,\n",
              "       0.2234699 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 447
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA5tMvLyVZWa",
        "outputId": "e1d08adb-bf33-4385-ba62-933b3b03c6e2"
      },
      "source": [
        "# Plot results for every element in the grid\n",
        "episodes_length = []\n",
        "for gate in gates_sequence[-100:]:\n",
        "  episodes_length.append(len(gate))\n",
        "\n",
        "np.mean(episodes_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "205.6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1EV0Y_jNcGf"
      },
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "grid = ParameterGrid({'eps': [0.4, 0.5, 0.6, 0.7, 0.8], 'step_size': [0.001, 0.01, 0.05, 0.1, 0.3]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6Lzc9oUxr1r"
      },
      "source": [
        "# TODO: for each episode monitor fidelity trend ==> is there learning or is it random?\n",
        "for params in grid:\n",
        "  eps = params['eps']\n",
        "  step_size = params['step_size']\n",
        "  start = [0.1, 0.1, 0.1, 0.985]\n",
        "  target = [0.5, 0.5, 0.5, 0.5]\n",
        "  env = LinearModel(QuantumState(start), QuantumState(target), TOLERANCE)\n",
        "  agent = LSTDAgent(env._num_gates, 4, env.get_obs(), step_size, eps)\n",
        "  mean_rewards, gates_sequence = run_experiment(env, agent, NUM_EPISODES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8FTCC1fMUS6"
      },
      "source": [
        "# Plot results for every element in the grid\n",
        "episodes_length = []\n",
        "for gate in gates_sequence:\n",
        "  episodes_length.append(len(gate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US6ZFLkaMxJh",
        "outputId": "c36ecf20-3b73-461a-8093-2e8c7dc1a0c7"
      },
      "source": [
        "np.mean(episodes_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "322.58"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 272
        }
      ]
    }
  ]
}